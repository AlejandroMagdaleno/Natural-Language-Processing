The base model has decent results but this could be due to the amount of data being used. After training using 80 percent of all data, we were able to get an accuracy of 97 percent and a loss of 20 percent. Currently, these are the main numbers that we are focused on and are trying to adjust. 97 percent for the accuracy is a great number but we want to make sure the model didn't overfit the data. This would be a problem that we could fix by adding in more data so that the model has more to see. If the model starts learning really well from little data, it will start to predict the next results almost as if it's remembering previous results. We want to avoid that problem down the road. By continuing that process of adding data and increasing the complexity of the model, we will also be able to bring down the loss further and even avoid underfitting. Underfitting would be where the model can't make good predictions of the data. Currently, this seems to be avoided which is good and allows us to focus on other aspects of the project. Work to predict on new test sets is still being made. Working with tensorflow datasets is new and we are looking into creating new test sets with these data types in order to give the model new articles to predict.