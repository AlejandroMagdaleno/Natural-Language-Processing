There were a few considerations when approaching this project. Two of the main ones were using pipelines provided by fasttext, spacy, and TensorFlow. Fastttext provided a very quick and easy route to create a supervised classification model for our text with its pipeline. The one problem with it however was trying to be flexible and account for complexity within the text. Due to this, TensorFlow ended up being the main library used in our text classification project. With the use of TensorFlow, we were able to label our data through the use of directories. This means that TensorFlow can detect class "a" and class "b" within the test and train directory folders of our data. As we get more data, TensorFlow is flexible enough to adjust to the increase in classes when we had more news stations to our training and test folders to add more classes later. Currently, the model takes in the raw text files that we put together and vectorizes the text into integer vectors. The model will create a vocabulary and frequency with these vectors in order to process the text embedding. After we setting up the input into integer vectors, a neural network is created through TensorFlow Keras. Currently, a sequential or feed-forward network is being used as the base model. This model can be adjusted if needed into other sorts of models like convolutional networks or recurrent networks. A neural network ended up being chosen here to provide flexibility and complex learning capabilities to the problem. Our neural network uses the adam algorithm as the optimizer which is a stochastic gradient descent algorithm provided by Keras. The loss function used is binary cross-entropy which is only being used while we have two different classes. This model still has plenty of adjustments being made as more data comes in. Such as changing hyper-parameters, learning rates, loss functions, optimizers, and the architecture of the neural network.