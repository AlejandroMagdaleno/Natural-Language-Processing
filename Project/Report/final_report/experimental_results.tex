The first model that was created was trained off of a significantly smaller dataset. The first dataset held less than 200 articles which were definitely affecting the performance of the model. The first accuracy and loss that came in were 97 percent and 20 percent respectively. However, it was hard to justify these numbers with the amount of data that we had so we increased the number of articles being used to around 20,000 articles. This ended bringing down our loss significantly to less than 2 percent and it put the accuracy up to around 98 to 99 percent depending on the run. Seeing a loss that low on our final runs brought up questions such as if the model was overfitting and generalizing too much. The next step to improve the model would be to continue tuning it even further. Hyperparameters can be changed as well as more layers can be implemented to provide more variety and complexity to the model. This way the model can pick up on more patterns that could possibly exist within the data. 