The data will need to be prepared first. The data consisting of the transcriptions will also include a label for it's output. A possible use for an encoder can be used for the target output value in order to categorize the outputs. After the data is organized and set, we will also need to use get some features from the data such as getting context through TF-IDF, topic features through context, or other methods like count vectors. After some initial features are received from the data, this is where model training will start to kick in. There are some routes that can be taken here from using traditional artificial intelligence strategies like naive bayes classifier, or going into deep learning models such as convolutional neural networks(CNN). The use of deep learning could serve a great advantage here becuase we can have the use of non-linearity. A deep learning model such as a CNN can take find and extract more patterns through training and eventually find the best model to solve the problem. In the case that a CNN is used. The first layers in the network would be used to embed the sentences into low dimensional vectors. Then we would run multiple convolutions or filters on the input data in order to create a feature map of the best patterns that are being found on the inputs. This will eventually start leading us to more accurate predictions. Aftewards, a max-pooling of the layers will come in which will attempt to calculate the maximum value in the feature maps from the previously filters layers. At the end of the model, a softmax classifier can be reached to give a target news station that the data came from. This is one example of an ideal model that can be used for our project. 