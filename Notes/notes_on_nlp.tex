\documentclass{book}
\begin{document}
\chapter{Introduction}
\chapter{Language Models}
\chapter{Supervised Machine Learning}
\chapter{Vector Semantics and Embedding}
\section{Basic Concepts}
\textbf{Distributional hypothesis}, linguistic hypothesis, words that occur in similar context tent to have similar meanings. The describes a link between how similar words are distributed and the meaning of the similarity. 
\textbf{Vector Semantics} is the tool that instantiates linguistic hypothesis or in other words the tool that shows when words are used in the same context. 
\textbf{Word embeddings} are representations of words in vectors so that similar words are in the neighboring vector spaces. 
\begin{quote}
	How tall is Mt. Everset? \\
	The official height of Mt. Everest is 29029 feet. 
	\emp{tall is similar to height}
\end{quote}
You compute the semantic similarity of words as the similarity of the embedded vectors. 
\chapter{Neural Network and Language Model}
\end{document}
